#include "FGRDiffusionThreads.hh"
#include <cassert>
#include "DiffusionUtils.hh"
#include "Anatomy.hh"
#include "Vector.hh"
#include "fastBarrier.hh"
#include <algorithm>
#include <cstdio>
#include "ThreadServer.hh"
#include "ThreadUtils.hh"
#include "PerformanceTimers.hh"

using namespace PerformanceTimers;
using namespace std;
using namespace FGRUtils;


FGRDiffusionThreads::FGRDiffusionThreads(const FGRDiffusionParms& parms,
                                         const Anatomy& anatomy,
                                         const ThreadTeam& threadInfo,
                                         const ThreadTeam& reactionThreadInfo)
: Diffusion(parms.diffusionScale_),
  nLocal_(anatomy.nLocal()),
  localGrid_(DiffusionUtils::findBoundingBox(anatomy, parms.printBBox_)),
  threadInfo_(threadInfo),
  reactionThreadInfo_(reactionThreadInfo)
{

   unsigned nx = localGrid_.nx();
   unsigned ny = localGrid_.ny();
   unsigned nz = localGrid_.nz();

   // This is a test
   for (unsigned ii=0; ii<anatomy.size(); ++ii)
   {
      Tuple globalTuple = anatomy.globalTuple(ii);
      Tuple ll = localGrid_.localTuple(globalTuple);
      assert(ll.x() >= 0 && ll.y() >= 0 && ll.z() >= 0);
      assert(ll.x() < nx && ll.y() < ny && ll.z() < nz);
   }
   // This has been a test

   mkOffsets(threadOffset_,     anatomy.nLocal(),  threadInfo_);
   mkOffsets(localCopyOffset_,  anatomy.nLocal(),  reactionThreadInfo_);
   mkOffsets(remoteCopyOffset_, anatomy.nRemote(), threadInfo_);
   

   
   weight_.resize(nx, ny, nz);
   A0_.resize(nx, ny, nz);
   VmBlock_.resize(nx, ny, nz);
   dVmBlock_.resize(nx, ny, nz);

   buildTupleArray(anatomy);
   buildBlockIndex(anatomy);

   int base = VmBlock_.tupleToIndex(1, 1, 1);
   offset_[ZZZ] = 0;
   offset_[PZZ] = VmBlock_.tupleToIndex(2, 1, 1) - base;
   offset_[ZPZ] = VmBlock_.tupleToIndex(1, 2, 1) - base;
   offset_[MZZ] = VmBlock_.tupleToIndex(0, 1, 1) - base;
   offset_[ZMZ] = VmBlock_.tupleToIndex(1, 0, 1) - base;
   offset_[PPZ] = VmBlock_.tupleToIndex(2, 2, 1) - base;
   offset_[MPZ] = VmBlock_.tupleToIndex(0, 2, 1) - base;
   offset_[MMZ] = VmBlock_.tupleToIndex(0, 0, 1) - base;
   offset_[PMZ] = VmBlock_.tupleToIndex(2, 0, 1) - base;
   offset_[ZZP] = VmBlock_.tupleToIndex(1, 1, 2) - base;
   offset_[ZZM] = VmBlock_.tupleToIndex(1, 1, 0) - base;
   offset_[ZPP] = VmBlock_.tupleToIndex(1, 2, 2) - base;
   offset_[ZPM] = VmBlock_.tupleToIndex(1, 2, 0) - base;
   offset_[ZMM] = VmBlock_.tupleToIndex(1, 0, 0) - base;
   offset_[ZMP] = VmBlock_.tupleToIndex(1, 0, 2) - base;
   offset_[PZP] = VmBlock_.tupleToIndex(2, 1, 2) - base;
   offset_[MZP] = VmBlock_.tupleToIndex(0, 1, 2) - base;
   offset_[MZM] = VmBlock_.tupleToIndex(0, 1, 0) - base;
   offset_[PZM] = VmBlock_.tupleToIndex(2, 1, 0) - base;

   faceNbrOffset_[0] = offset_[MZZ];
   faceNbrOffset_[1] = offset_[ZMZ];
   faceNbrOffset_[2] = offset_[PZZ];
   faceNbrOffset_[3] = offset_[ZPZ];
   faceNbrOffset_[4] = offset_[ZZM];
   faceNbrOffset_[5] = offset_[ZZP];

   precomputeCoefficients(anatomy);
}


void FGRDiffusionThreads::updateLocalVoltage(ro_mgarray_ptr<double> VmLocal_managed)
{
   startTimer(FGR_ArrayLocal2MatrixTimer);
   ro_array_ptr<double> VmLocal = VmLocal_managed.useOn(CPU);
   int tid = reactionThreadInfo_.teamRank();
   unsigned begin = localCopyOffset_[tid];
   unsigned end   = localCopyOffset_[tid+1];
   for (unsigned ii=begin; ii<end; ++ii)
   {
      int index = blockIndex_[ii];
      VmBlock_(index) = VmLocal[ii];
   }
   stopTimer(FGR_ArrayLocal2MatrixTimer);
}

void FGRDiffusionThreads::updateRemoteVoltage(ro_mgarray_ptr<double> VmRemote_managed)
{
   startTimer(FGR_ArrayRemote2MatrixTimer);
   ro_array_ptr<double> VmRemote = VmRemote_managed.useOn(CPU);
   int tid = threadInfo_.teamRank();
   unsigned begin = remoteCopyOffset_[tid];
   unsigned end   = remoteCopyOffset_[tid+1];
   unsigned* bb = &blockIndex_[nLocal_];
   for (unsigned ii=begin; ii<end; ++ii)
   {
      int index = bb[ii];
      VmBlock_(index) = VmRemote[ii];
   }
   stopTimer(FGR_ArrayRemote2MatrixTimer);
}




void FGRDiffusionThreads::calc(rw_mgarray_ptr<double> /*dVm_managed*/)
{
   startTimer(FGR_StencilTimer);

   int tid = threadInfo_.teamRank();
   int begin = threadOffset_[tid];
   int end   = threadOffset_[tid+1];
   
   for (int iCell=begin; iCell<end; ++iCell)
   {
      int ib = blockIndex_[iCell];
      
      double* phi = & (VmBlock_(ib));
      const WeightType *A = weight_(ib).A;
      dVmBlock_(ib) = A0_(ib) * (*(phi+offset_[0]));
      for (unsigned ii=1; ii<19; ++ii)
         dVmBlock_(ib) += A[ii] * ( *(phi+offset_[ii]));

      // calculate in array layout
//      dVm[iCell] = 0.0;
//       for (unsigned ii=0; ii<19; ++ii)
//          dVm[iCell] += A[ii] * ( *(phi+offset_[ii]));
//       dVm[iCell] *= diffusionScale_;
   }
   stopTimer(FGR_StencilTimer);
}

/** We're building the localTuple array only for local cells.  We can't
 * do stencil operations on remote particles so we shouldn't need
 * tuples.  We can use block indices instead.
 */
void FGRDiffusionThreads::buildTupleArray(const Anatomy& anatomy)
{
   localTuple_.resize(anatomy.nLocal(), Tuple(0,0,0));
   for (unsigned ii=0; ii<anatomy.nLocal(); ++ii)
   {
      Tuple globalTuple = anatomy.globalTuple(ii);
      localTuple_[ii] = localGrid_.localTuple(globalTuple);
      assert(localTuple_[ii].x() > 0);
      assert(localTuple_[ii].y() > 0);
      assert(localTuple_[ii].z() > 0);
      assert(localTuple_[ii].x() < localGrid_.nx()-1);
      assert(localTuple_[ii].y() < localGrid_.ny()-1);
      assert(localTuple_[ii].z() < localGrid_.nz()-1);
   }
}

void FGRDiffusionThreads::buildBlockIndex(const Anatomy& anatomy)
{
   blockIndex_.resize(anatomy.size());
   for (unsigned ii=0; ii<anatomy.size(); ++ii)
   {
      Tuple globalTuple = anatomy.globalTuple(ii);
      Tuple ll = localGrid_.localTuple(globalTuple);
      blockIndex_[ii] = VmBlock_.tupleToIndex(ll.x(), ll.y(), ll.z());
   }
}

void FGRDiffusionThreads::precomputeCoefficients(const Anatomy& anatomy)
{
   unsigned nx = localGrid_.nx();
   unsigned ny = localGrid_.ny();
   unsigned nz = localGrid_.nz();
   unsigned nxGlobal = anatomy.nx();
   unsigned nyGlobal = anatomy.ny();
   unsigned nzGlobal = anatomy.nz();
   Vector hInv(1.0/anatomy.dx(), 1.0/anatomy.dy(), 1.0/anatomy.dz());
   Vector h(anatomy.dx(), anatomy.dy(), anatomy.dz());
   double gridCellVolume = h[0]*h[1]*h[2];
   
   SymmetricTensor sigmaZero = {0};
   Array3d<SymmetricTensor> sigmaBlk(nx, ny, nz, sigmaZero);
   Array3d<int> tissueBlk(nx, ny, nz, 0);

   const vector<AnatomyCell>& cell = anatomy.cellArray();
   for (unsigned ii=0; ii<anatomy.size(); ++ii)
   {
      unsigned ib = blockIndex_[ii];
      sigmaBlk(ib) = anatomy.conductivity(ii);
      tissueBlk(ib) = isTissue(anatomy.cellType(ii));
   }

   for (unsigned ii=0; ii<weight_.size(); ++ii)
      for (unsigned jj=0; jj<19; ++jj)
         weight_(ii).A[jj] = 0.0;

   for (unsigned iCell=0; iCell<anatomy.nLocal(); ++iCell)
   {
      unsigned ib = blockIndex_[iCell];
      int tissue[19] = {0};
      mkTissueArray(tissueBlk, ib, tissue);
      
      for (unsigned iFace=0; iFace<6; ++iFace)
      {
         unsigned faceNbrIndex = ib+faceNbrOffset_[iFace];
         if (tissueBlk(faceNbrIndex) == 0)
            continue;

         Vector sigmaTimesS = f1(ib, iFace, h, sigmaBlk)/gridCellVolume;
         double gradPhi[3][19] = {0};
         f2(iFace, tissue, gradPhi);
         
         for (unsigned ii=0; ii<19; ++ii)
            for (unsigned jj=0; jj<3; ++jj)
               weight_(ib).A[ii] += sigmaTimesS[jj] * gradPhi[jj][ii] * hInv[jj];
      }
      double sum = weight_(ib).A[0];
      for (unsigned ii=1; ii<19; ++ii)
      {
         sum += weight_(ib).A[ii];
         A0_(ib) -= weight_(ib).A[ii];
      }
      assert(abs(sum) < weightSumTolerance);
   }
//   printAllWeights(tissueBlk);
}

void FGRDiffusionThreads::mkTissueArray(
   const Array3d<int>& tissueBlk, int ib, int* tissue)
{
   for (unsigned ii=0; ii<19; ++ii)
      tissue[ii] = tissueBlk(ib + offset_[ii]);
}


Vector FGRDiffusionThreads::f1(int ib, int iFace, const Vector& h,
                        const Array3d<SymmetricTensor>& sigmaBlk)
{
   SymmetricTensor
      sigma = (sigmaBlk(ib) + sigmaBlk(ib+faceNbrOffset_[iFace])) / 2.0;
   Vector S(0, 0, 0);
   switch (iFace)
   {
     case 0:
      S[0] = -h[1]*h[2];
      break;
     case 1:
      S[1] = -h[0]*h[2];
      break;
     case 2:
      S[0] = h[1]*h[2];
      break;
     case 3:
      S[1] = h[0]*h[2];
      break;
     case 4:
      S[2] = -h[0]*h[1];
      break;
     case 5:
      S[2] = h[0]*h[1];
      break;
      
     default:
      assert(false);
   }
   return sigma * S;
}


void FGRDiffusionThreads::printAllWeights(const Array3d<int>& tissue)
{
   for (unsigned ii=0; ii<localTuple_.size(); ++ii)
   {
      Tuple globalTuple = localGrid_.globalTuple(localTuple_[ii]);
      int xx = localTuple_[ii].x();
      int yy = localTuple_[ii].y();
      int zz = localTuple_[ii].z();

      printf("DiffusionWeight: %5d %5d %5d %4d"
             " %20.12e %20.12e %20.12e %20.12e %20.12e %20.12e"
             " %20.12e %20.12e %20.12e %20.12e %20.12e %20.12e"
             " %20.12e %20.12e %20.12e %20.12e %20.12e %20.12e"
             " %20.12e\n",
             globalTuple.x(), globalTuple.y(), globalTuple.z(),
             tissue(xx, yy, zz),
             weight_(xx, yy, zz).A[0],
             weight_(xx, yy, zz).A[1],
             weight_(xx, yy, zz).A[2],
             weight_(xx, yy, zz).A[3],
             weight_(xx, yy, zz).A[4],
             weight_(xx, yy, zz).A[5],
             weight_(xx, yy, zz).A[6],
             weight_(xx, yy, zz).A[7],
             weight_(xx, yy, zz).A[8],
             weight_(xx, yy, zz).A[9],
             weight_(xx, yy, zz).A[10],
             weight_(xx, yy, zz).A[11],
             weight_(xx, yy, zz).A[12],
             weight_(xx, yy, zz).A[13],
             weight_(xx, yy, zz).A[14],
             weight_(xx, yy, zz).A[15],
             weight_(xx, yy, zz).A[16],
             weight_(xx, yy, zz).A[17],
             weight_(xx, yy, zz).A[18]);
   }
}

